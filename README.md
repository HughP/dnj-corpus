# dnj-corups
A small corpus of a local newspaper

## Language Description

Language ISO639-3: dnj

Language Name: Eastern Dan

Orthography version: 3

## Corpus Description
Linux Command Line:
`wc -l -w -m`

Lines  | Words  |  Characters
--|---|--
  11686 | 46192  |  221389


UnicodeCharacterCount Stats:

U+0009		194

U+000A		11686

U+000C		88

U+0020		36545

U+0021	!	25

U+0022	"	1576

U+0027	'	2363

U+0028	(	309

U+0029	)	308

U+002B	+	86

U+002C	,	2212

U+002C+0308	,̈	3

U+002D	-	15242

U+002E	.	2128

U+002F	/	13

U+0030	0	689

U+0031	1	208

U+0032	2	323

U+0033	3	93

U+0034	4	80

U+0035	5	147

U+0036	6	63

U+0037	7	119

U+0038	8	198

U+0039	9	96

U+003A	:	230

U+003B	;	29

U+003C	<	72

U+003D	=	2659

U+003E	>	72

U+003F	?	85

U+0041	A	590

U+0042	B	260

U+0043	C	12

U+0044	D	432

U+0045	E	84

U+0046	F	35

U+0047	G	247

U+0048	H	22

U+0049	I	48

U+004A	J	9

U+004B	K	807

U+004C	L	106

U+004D	M	397

U+004E	N	248

U+004F	O	36

U+0050	P	201

U+0052	R	7

U+0053	S	311

U+0054	T	153

U+0055	U	25

U+0056	V	92

U+0057	W	311

U+0059	Y	448

U+005A	Z	236

U+005B	[	3

U+005D	]	3

U+005F	_	1

U+0061	a	15986

U+0062	b	4805

U+0063	c	350

U+0064	d	6289

U+0065	e	3136

U+0066	f	272

U+0067	g	5460

U+0068	h	7528

U+0069	i	4773

U+006A	j	58

U+006B	k	6592

U+006C	l	2387

U+006D	m	2458

U+006E	n	8866

U+006F	o	5156

U+0070	p	2294

U+0071	q	83

U+0072	r	1328

U+0073	s	3972

U+0074	t	2249

U+0075	u	4260

U+0076	v	302

U+0077	w	4958

U+0078	x	69

U+0079	y	3564

U+007A	z	992

U+00AB	«	18

U+00B0	°	1

U+00BB	»	16

U+00CB	Ë	37

U+00D6	Ö	26

U+00DC	Ü	29

U+00E7	ç	17

U+00E8	è	178

U+00E9	é	85

U+00EA	ê	24

U+00EB	ë	4799

U+00EE	î	3

U+00F6	ö	6762

U+00FB	û	22

U+00FC	ü	3299

U+0186	Ɔ	45

U+0190	Ɛ	52

U+0254	ɔ	4259

U+025B	ɛ	6619

U+0269	ɩ	517

U+028B	ʋ	584

U+028B+0308	ʋ̈	260

U+0308	̈	5

U+03CB	ϋ	697

U+2013	–	985

U+2018	‘	7305

U+2019	’	642

U+201A	‚	5

U+201C	“	2449

U+201D	”	59

U+2026	…	5

U+2039	‹	30

U+203A	›	30

U+FFF9	￹	10

## Provenance
Valentin Vydrin `vydrine[at]gmail[dot]com`  Provided the corpus as a series of .doc files.

Hugh Paterson III converted those to PDFs and then used pdftotext to extract the text. The text was then converted to a single file `mass-text.txt` useing the following commands.

using this command on linux  `for f in gweta*.pdf; do pdftotext $f mass-text_$f.txt; done` and then  `cat mass-text_gweta*.txt >> mass-text.txt`

## File types and purpose

`gweta*.doc` these are the original files provided by VV.
`gweta*.pdf`these are PDFs generated my MS Word by Rebecca Paterson from files provided by VV.
`gweta*.txt` these files are generated by Hugh Paterson using `pdftotext`.

`*-sfm.txt` files have a hand coded structure to them that includes making for things like newspaper title, volume, date, tagline, article, heading 1, heading 2, and text of article:

```
\newspaper Pamɛbhamɛ
\volume-eng 001
\volume-or "Nimlʋʋ : 00x---
\date 2005 'Zë Zë -kwɛ
\tagline "su –bha ‘sëëdhɛ -mü "Gwɛɛtaawo
\body
\article 1
\heading 1
\heading 2
\p 1
```

## Research Notes

### Working with custom character attributes


Non-standard components (from an ASCII perspective):
1. It has some greek/IPA characters in the orthography. This does not match standard [a-zA-Z] classes. [ë, ɛ, ɔ, ʋ, ö,]
2. It has tone diacritics. This also does not match standard [a-zA-Z] classes.
3. It uses punctuation marks to indicate tone. This does not match standard [a-zA-Z] classes. And means that they need to be included in the word, and excluded from punctuation classes.

Tasks:

1. I would like to count unique stems and their variations.

How do I expect to count variations? the following would all be variations. (I know that they might not be the same "lexical" word but that is a small aside for me right now.)
=ban-
=ban`
-ban`
"ban
'ban
Basically where we only count segmental features.

2. I would like to get a unique string count which assumes that the language's punctuation is the border of words.

If I say that space is the word break then words punctuation is included in words.

3. I would like to count overtly marked tonal melodies. That is something like:

```
break strings into segments according to custom word forming rules.
 search between word forming characters for characters in tonal indicator class until reaching the last word forming character in the string
   Return tonal characters as a set.
Count unique sets
Write results to table
```

  Results| Count  |  Phonology
--|---|--
  =-|  45 |  H+ M
  =` | 37  |  H+ L
  -` |  22 |  L H
  "|  56 |  H L

Results : (45 set =-, 37 =\`, 22 -\`, 56 ", etc.)

Challenge: Given the nature of the custom (non-standard Unicode) properties attributed to this orthography I need to both expand the alpha numeric range of character tools like RegEx [:alpha:] and then also limit classes like "Punctuation".

Here is what I have tried, but It is not getting me where I need to go.



Corpus location:




Word component characters:


Punctuation characters:


Other Characters used:



### Tool for the Job Notes

#### LibreOffice headless

`libreoffice --headless --convert-to "txt:Text (encoded):UTF8" mydocument.doc`
* Source:
 https://stackoverflow.com/questions/5671988/how-to-extract-just-plain-text-from-doc-docx-files-unix
* _Note_:
This works for most things in the .doc files, but did not get all headings. or all content from text boxes.

#### Antiword
* Source:
 http://www.winfield.demon.nl/#Programmer
* _Note_:
 puts in a lot of character for charts. (makes ASCII charts), when counting characters this will throw off character counts.

#### textract
* Source: https://textract.readthedocs.io/en/stable/installation.html#ubuntu-debian
* _Note_:
 This is just a python wraper around several tools, for .doc antiword was the tool used. So why not just use antiword?

#### pandoc
* Source:
* _Note_:
Did not work with .doc files.

#### doctotext
* Source:  http://silvercoders.com/en/products/doctotext/
* _Note_:
Needed local compiling directions were not clear. needed more know how to accomplish this.

```
cat target.file | tr '[:space:]' '[####  n*]' | grep -v "^####  s*$" | sort | uniq -c | sort -bnr
cat target.file | tr '[:space:]' '[####  n*]' | grep -v "^####  s*$" | sort | uniq -c | sort -bnr | top
cat target.file  | tr '[:space:]' '[####  n*]' | grep -v "^####  s*$" | sort | uniq -c | sort -bnr | top | head -20
cat target.file  | tr '[:space:]' '[####  n*]' | grep -v "^####  s*$" | sort | uniq -c | sort -bnr | head -20
  grep -o -c phonology target.file
  grep -o -c Phonology target.file
 sed -e 's/[^[:alpha:]]/ /g' text_to_analize.txt | tr '####  n' " " |  tr -s " " | tr " " '####  n'| tr 'A-Z' 'a-z' | sort | uniq -c | sort -nr | nl
  sed -e 's/[^[:alpha:]]/ /g' target.file | tr '####  n' " " |  tr -s " " | tr " " '####  n'| tr 'A-Z' 'a-z' | sort | uniq -c | sort -nr | nl

pdftotext target.file
  sed -e 's/[^[:alpha:]]/ /g' target.file | tr '####  n' " " |  tr -s " " | tr " " '####  n'| tr 'A-Z' 'a-z' | sort | uniq -c | sort -nr | nl

  ```
